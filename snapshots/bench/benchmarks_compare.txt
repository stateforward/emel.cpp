# ref=94b0200a01a753eff5897dab9311f51a7bc1c62f
# toolchain=/opt/homebrew/bin/zig
batch/splitter_equal emel.cpp 1571.208 ns/op, llama.cpp 6402.871 ns/op, ratio=0.245x
batch/splitter_seq emel.cpp 1432.146 ns/op, llama.cpp 2653.508 ns/op, ratio=0.540x
batch/splitter_simple emel.cpp 742.508 ns/op, llama.cpp 2275.896 ns/op, ratio=0.326x
buffer/allocator_alloc_graph emel.cpp 17.113 ns/op, llama.cpp 54.221 ns/op, ratio=0.316x
buffer/allocator_full emel.cpp 39.987 ns/op, llama.cpp 251.492 ns/op, ratio=0.159x
buffer/allocator_reserve_n emel.cpp 20.696 ns/op, llama.cpp 426.308 ns/op, ratio=0.049x
jinja/parser_long emel.cpp 31008.346 ns/op, llama.cpp 50653.367 ns/op, ratio=0.612x
jinja/parser_short emel.cpp 408.267 ns/op, llama.cpp 499.417 ns/op, ratio=0.817x
jinja/renderer_long emel.cpp 91826.917 ns/op, llama.cpp 232903.558 ns/op, ratio=0.394x
jinja/renderer_short emel.cpp 1429.925 ns/op, llama.cpp 3899.846 ns/op, ratio=0.367x
memory/coordinator_recurrent_full emel.cpp 3818.421 ns/op, llama.cpp 5427.163 ns/op, ratio=0.704x
tokenizer/preprocessor_bpe_long emel.cpp 15788.421 ns/op, llama.cpp 16948.079 ns/op, ratio=0.932x
tokenizer/preprocessor_bpe_short emel.cpp 498.583 ns/op, llama.cpp 727.483 ns/op, ratio=0.685x
tokenizer/preprocessor_plamo2_long emel.cpp 3074.358 ns/op, llama.cpp 4663.188 ns/op, ratio=0.659x
tokenizer/preprocessor_plamo2_short emel.cpp 2439.450 ns/op, llama.cpp 3594.912 ns/op, ratio=0.679x
tokenizer/preprocessor_rwkv_long emel.cpp 3167.096 ns/op, llama.cpp 4548.988 ns/op, ratio=0.696x
tokenizer/preprocessor_rwkv_short emel.cpp 2448.875 ns/op, llama.cpp 3436.583 ns/op, ratio=0.713x
tokenizer/preprocessor_spm_long emel.cpp 3041.883 ns/op, llama.cpp 4731.308 ns/op, ratio=0.643x
tokenizer/preprocessor_spm_short emel.cpp 2355.250 ns/op, llama.cpp 3643.054 ns/op, ratio=0.647x
tokenizer/preprocessor_ugm_long emel.cpp 3117.054 ns/op, llama.cpp 4778.796 ns/op, ratio=0.652x
tokenizer/preprocessor_ugm_short emel.cpp 2346.454 ns/op, llama.cpp 3618.200 ns/op, ratio=0.649x
tokenizer/preprocessor_wpm_long emel.cpp 3025.596 ns/op, llama.cpp 4807.304 ns/op, ratio=0.629x
tokenizer/preprocessor_wpm_short emel.cpp 2352.137 ns/op, llama.cpp 3642.429 ns/op, ratio=0.646x
